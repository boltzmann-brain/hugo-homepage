---
title: "This Week in Responsible AI: March 9, 2023"
date: 2023-03-08T23:04:09-05:00
draft: true
---

## Law/Policy

- BetterHelp [settles with FTC](https://www.theverge.com/2023/3/2/23622227/betterhelp-customer-data-advertising-privacy-facebook-snapchat) for leaking personal health information to social media platforms

- [Ten Legal and Business Risks of Chatbots and Generative AI](https://techpolicy.press/ten-legal-and-business-risks-of-chatbots-and-generative-ai/?mc_cid=ca3f4a55ff&mc_eid=f14ac890df)

- [What the EU's Digital Services Act and Digital Markets Act will do](https://www.technologyreview.com/2023/03/06/1069391/safer-internet-dsa-dma-eu/?truid=&utm_source=the_download&utm_medium=email&utm_campaign=the_download.unpaid.engagement&utm_term=Active%20Qualified&utm_content=03-07-2023&mc_cid=58d6e24b80&mc_eid=f14ac890df)

- [Responsible AI: What Does It Take to Turn Principles into Practice?](https://www.govtech.com/artificial-intelligence/responsible-ai-what-does-it-take-to-turn-principles-into-practice)

- [Is it time to hit the pause button on AI?](https://michellerempelgarner.substack.com/p/is-it-time-to-hit-the-pause-button)

- Suresh Venkatasubramanian's [remarks](https://cntr.substack.com/p/remarks-before-the-senate-homeland?utm_source=twitter&utm_campaign=auto_share&r=a1fb3) at a Senate committee hearing on AI: Risks and Opportunities

- ["73% of ML practitioners agree that the AI Bill of Rights should be mandatory by law vs. opt-in."](https://www.helpnetsecurity.com/2023/03/03/ai-bill-of-rights/)


## Tools/Resources

- [Causal Dependence Plots for Interpretable Machine Learning](https://arxiv.org/abs/2303.04209)

- [Charting the Sociotechnical Gap in Explainable AI](https://arxiv.org/abs/2302.00799)

- [Improving Transparency in AI Language Models: A Holistic Evaluation](https://hai.stanford.edu/foundation-model-issue-brief-series?utm_source=twitter&utm_medium=social&utm_content=Stanford%20HAI_twitter_StanfordHAI_202303070801_sf175668114&utm_campaign=&sf175668114=1)

- [FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling](https://fair-ensemble.github.io)

- [Ethical guidelines for developing the Diffusers library](https://huggingface.co/blog/ethics-diffusers)

## AI harms

- LLaMA leak:
    - ["This represents a kind of 'race to the bottom' in terms of moving from maximal control to maximal diffusion of models"](https://cyberscoop.com/meta-large-language-model-available-online/)
    - [The LLaMA is out of the bag. Should we expect a tidal wave of disinformation?](https://aisnakeoil.substack.com/p/the-llama-is-out-of-the-bag-should?utm_source=substack&utm_medium=email)
    - ['The LLaMA leak is... interesting because it plays into an ongoing ideological struggle in the wider world of AI: the battle between "closed" and "open" systems.'](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse)

- ['Alarmingly, roughly half of all "clinical language models"... aren't actually being validated on clinical text, and thus they may be greatly overestimating their performance in a hospital setting.'](https://hai.stanford.edu/news/shaky-foundations-foundation-models-healthcare?utm_source=Stanford+HAI&utm_campaign=937f47d215-Mailchimp_HAI_Newsletter_March+2023_1_General&utm_medium=email&utm_term=0_aaf04f4a4b-4ddec8b848-%5BLIST_EMAIL_ID%5D)

- [How the first chatbot predicted the dangers of AI more than 50 years ago](https://www.vox.com/future-perfect/23617185/ai-chatbots-eliza-chatgpt-bing-sydney-artificial-intelligence-history?mc_cid=a765828b9b&mc_eid=f14ac890df) 

- ["AI advances that increase productivity are likely to result in increasing the already disproportionate burden on everyone but a privileged elite"](https://www.cigionline.org/articles/claims-that-ai-productivity-will-save-us-are-neither-new-nor-true/)

- [Suspicion Machines](https://www.lighthousereports.com/investigation/suspicion-machines/): welfare surveillance algorithms across the globe. 

- ["In all these types of algorithmic programs, the algorithm, by definition, is seeking to measure and assess a person's performance against a perceived or presumed norm. And, by definition, disabled people fall outside the norm"](https://just-tech.ssrc.org/articles/disability-justice-and-tech-a-conversation-with-lydia-x-z-brown/)

- [Algorithmic classifications in credit marketing: How marketing shapes inequalities](https://journals.sagepub.com/doi/10.1177/14705931231160828)

- [Former Google AI Ethicist Calls Bard and alike 'Bulls***' Generators](https://analyticsindiamag.com/former-google-ai-ethicist-calls-bard-and-alike-bulls-generators/)

- ['Getting a scientific article peer-reviewed takes *at least* 3-6 months, whereas the speed at which half-baked AI systems are being deployed at in the real world is much, much faster.'](https://twitter.com/SashaMTL/status/1631239207577436161?s=20)

- ['algorithms trained on automatic behaviors can misunderstand the prejudice of users: the more automatic the behavior, the greater the error.'](https://www.nber.org/papers/w30981)


## Privacy

- Data brokers sell Americans' mental health data ([PDF](https://techpolicy.sanford.duke.edu/wp-content/uploads/sites/4/2023/02/Kim-2023-Data-Brokers-and-the-Sale-of-Americans-Mental-Health-Data.pdf?mc_cid=ca3f4a55ff&mc_eid=f14ac890df))

- [Mental health startup Cerebral has admitted to disclosing users' sensitive health data in violation of HIPAA.](https://twitter.com/themarkup/status/1633206129114136578?s=20)

- ['Requiring people, including children and teens, to trade their personal data in return for access to essential services "is not a real choice," he said, because of how pervasive digital communication is today.'](https://www.adexchanger.com/privacy/a-core-concept-of-ad-industry-self-regulation-is-under-siege/)


## Generative models

- ["If you believe that AI is very risky, as many experts do and as Altman himself says he does, it might seem strange to rush new and more powerful models to release as fast as possible."](https://www.vox.com/future-perfect/23619354/openai-chatgpt-sam-altman-artificial-intelligence-regulation-sydney-microsoft-ai-safety?mc_cid=430a4cefa1&mc_eid=f14ac890df)

- [U.S. special forces want to use deepfakes for psy-ops](https://theintercept.com/2023/03/06/pentagon-socom-deepfake-propaganda/?utm_source=substack&utm_medium=email)

- ["Experts said that when a government uses an AI tool, it should be held to a different standard than when a private company debuts one."](https://www.semafor.com/article/03/03/2023/governments-using-chatgpt-bots)

- [ChatGPT and large language model bias](https://www.cbsnews.com/news/chatgpt-large-language-model-bias-60-minutes-2023-03-05/)